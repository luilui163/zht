1 Stochastic simulation	28
	1.1 Introduction	28
	1.2 Generation of discrete random quantities	29
		1.2.1 Bernoulli distribution	30
		1.2.2 Binomial distribution	30
		1.2.3 Geometric and negative binomial distribution	31
		1.2.4 Poisson distribution	31
	1.3 Generation of continuous random quantities	32
		1.3.1 Probability integral transform	32
		1.3.2 Bivariate techniques	33
		1.3.3 Methods based on mixtures	36
	1.4 Generation of random vectors and matrices	39
		1.4.1 Multivariate normal distribution	40
		1.4.2 Wishart distribution	42
		1.4.3 Multivariate Student¡¯s t distribution	43
	1.5 Resampling methods	44
		1.5.1 Rejection method	44
		1.5.2 Weighted resampling method	49
		1.5.3 Adaptive rejection method	51
	1.6 Exercises	53
2 Bayesian inference	60
	2.1 Introduction	60
	2.2 Bayes¡¯ theorem	60
		2.2.1 Prior, posterior and predictive distributions	61
		2.2.2 Summarizing the information	66
	2.3 Conjugate distributions	68
		2.3.1 Conjugate distributions for the exponential family	70
		2.3.2 Conjugacy and regression models	74
		2.3.3 Conditional conjugacy	77
	2.4 Hierarchical models	79
	2.5 Dynamic models	82
		2.5.1 Sequential inference	83
		2.5.2 Smoothing	84
		2.5.3 Extensions	86
	2.6 Spatial models	87
	2.7 Model comparison	91
	2.8 Exercises	93
3 Approximate methods of inference	100
	3.1 Introduction	100
	3.2 Asymptotic approximations	101
		3.2.1 Normal approximations	102
		3.2.2 Mode calculation	105
		3.2.3 Standard Laplace approximation	107
		3.2.4 Exponential form Laplace approximations	109
	3.3 Approximations by Gaussian quadrature	112
	3.4 Monte Carlo integration	114
	3.5 Methods based on stochastic simulation	117
		3.5.1 Bayes¡¯ theorem via the rejection method	119
		3.5.2 Bayes¡¯ theorem via weighted resampling	120
		3.5.3 Application to dynamic models	123
	3.6 Exercises	125
4 Markov chains	132
	4.1 Introduction	132
	4.2 Definition and transition probabilities	133
	4.3 Decomposition of the state space	137
	4.4 Stationary distributions	140
	4.5 Limiting theorems	143
	4.6 Reversible chains	146
	4.7 Continuous state spaces	148
		4.7.1 Transition kernels	148
		4.7.2 Stationarity and limiting results	150
	4.8 Simulation of a Markov chain	151
	4.9 Data augmentation or substitution sampling	154
	4.10 Exercises	155
5 Gibbs sampling	160
	5.1 Introduction	160
	5.2 Definition and properties	161
	5.3 Implementation and optimization 148531 Forming the sample	167
		5.3.2 Scanning strategies	169
		5.3.3 Using the sample	170
		5.3.4 Reparametrization	171
		5.3.5 Blocking	174
		5.3.6 Sampling from the full conditional distributions	175
	5.4 Convergence diagnostics	176
		5.4.1 Rate of convergence	177
		5.4.2 Informal convergence monitors	178
		5.4.3 Convergence prescription	180
		5.4.4 Formal convergence methods	183
	5.5 Applications	188
		5.5.1 Hierarchical models	188
		5.5.2 Dynamic models	191
		5.5.3 Spatial models	195
	5.6 MCMC-based software for Bayesian modeling	197
	5.7 Exercises	203
6 Metropolis-Hastings algorithms	210
	6.1 Introduction	210
	6.2 Definition and properties	212
	6.3 Special cases	217
		6.3.1 Symmetric chains	217
		6.3.2 Random walk chains	217
		6.3.3 Independence chains	218
		6.3.4 Other forms	223
	6.4 Hybrid algorithms	224
		6.4.1 Componentwise transition	225
		6.4.2 Metropolis within Gibbs	230
		6.4.3 Blocking	233
		6.4.4 Reparametrization	235
	6.5 Applications	236
		6.5.1 Generalized linear mixed models	236
		6.5.2 Dynamic linear models	242
		6.5.3 Dynamic generalized linear models	245
		6.5.4 Spatial models	250
	6.6 Exercises	253
7 Further topics in M C M C	256
	7.1 Introduction	256
	7.2 Model adequacy	256
		7.2.1 Estimates of the predictive likelihood	257
		7.2.2 Uses of the predictive likelihood	267
		7.2.3 Deviance information criterion	272
	7.3 Model choice: MCMC over model and parameter spaces	276
		7.3.1 Markov chain for supermodels	277
		7.3.2 Markov chain with jumps	280
		7.3.3 Further issues related to RJMCMC algorithms	289
	7.4 Convergence acceleration	290
		7.4.1 Alterations to the chain	290
		7.4.2 Alterations to the equilibrium distribution	297
		7.4.3 Auxiliary variables	301
	7.5 Exercises	303
